# Location to save Q-table after training
q_table_filename: q_table.npy

# Environment parameters
cell_size: 300
env_width: 3
env_length: 3
env_n_agents: 2
env_n_pickers: 1
env_n_objects: 1
env_enable_rendering: False

# When playing the trained policy
num_play_episodes: 1
num_max_steps: 50
video_save_path: ${hydra:runtime.cwd}/videos/

# Training parameters
episodes: 20000
max_steps_per_episode: 50
discount_factor: 0.99
min_exploration: 0.01
exploration_rate: 1.0
exploration_decay: 0.995
min_learning_rate: 0.01
learning_rate: 0.1
learning_rate_decay: 0.995

# set the working dir to the current dir
hydra:
  run:
    dir: .

